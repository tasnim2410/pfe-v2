{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bed2684",
   "metadata": {},
   "source": [
    "searching + cleaning + fetching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a954ee08",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DATABASE_URL not set. Put it in .env to match the backend (Postgres).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m DATABASE_URL \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABASE_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATABASE_URL:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABASE_URL not set. Put it in .env to match the backend (Postgres).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(DATABASE_URL)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Helpers\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DATABASE_URL not set. Put it in .env to match the backend (Postgres)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "TECH_TERMS = [\n",
    "    \"lithium-ion battery\",\n",
    "    \"fuel cell\",\n",
    "    \"hydrogen storage\",\n",
    "    \"solar photovoltaic\",\n",
    "    \"wind turbine\",\n",
    "    \"3D printing\",\n",
    "    \"computer vision\",\n",
    "    \"wireless charging\",\n",
    "    \"radar sensor\",\n",
    "    \"autonomous driving\",\n",
    "]\n",
    "\n",
    "# Tail truncation (because of delays / incomplete years)\n",
    "PATENT_TAIL_TRUNC = 3   # drop last 3 years of patent data\n",
    "PUB_TAIL_TRUNC    = 1   # drop last 1 year of publication data\n",
    "\n",
    "MAX_RESULTS_PATENTS = 600\n",
    "REQUEST_TIMEOUT = 120\n",
    "\n",
    "# DB connection – expects DATABASE_URL in env (same as the Flask app)\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "if not DATABASE_URL:\n",
    "    raise RuntimeError(\"DATABASE_URL not set. Put it in .env to match the backend (Postgres).\")\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def load_backend_port():\n",
    "    \"\"\"\n",
    "    Try several locations; user said '/backend_prot.txt'.\n",
    "    The app writes to 'frontend/public/backend_port.txt' and 'shared_path.txt' too.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"/backend_prot.txt\",\n",
    "        \"backend_prot.txt\",\n",
    "        \"shared_path.txt\",\n",
    "        os.path.join(\"frontend\", \"public\", \"backend_port.txt\"),\n",
    "        os.getenv(\"NEXT_PUBLIC_BACKEND_PORT\", \"\").strip() or None\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if not c:\n",
    "            continue\n",
    "        if os.path.isfile(c):\n",
    "            try:\n",
    "                return int(open(c, \"r\").read().strip())\n",
    "            except Exception:\n",
    "                pass\n",
    "        # If it's already a number-like string from env:\n",
    "        try:\n",
    "            return int(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(\"Could not determine backend port from any known path/env.\")\n",
    "\n",
    "def patents_query_payload(term: str) -> dict:\n",
    "    \"\"\"\n",
    "    Build the JSON for /api/search_ops (OPS-based).\n",
    "    Uses a simple single-keyword group on 'title' to keep it robust.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"group1\": {\n",
    "                \"type\": \"group\",\n",
    "                \"operator\": \"AND\",\n",
    "                \"keywords\": [\n",
    "                    {\"type\": \"keyword\", \"word\": term, \"rule_op\": \"any\", \"field\": \"title\"}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"max_results\": MAX_RESULTS_PATENTS\n",
    "    }\n",
    "\n",
    "def post_json(url: str, payload: dict):\n",
    "    r = requests.post(url, json=payload, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_publications_by_year() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query the research_data3 table directly and aggregate by year.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT year::int AS year, COUNT(*)::int AS count\n",
    "        FROM research_data3\n",
    "        WHERE year IS NOT NULL\n",
    "        GROUP BY year\n",
    "        ORDER BY year\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql, engine)\n",
    "\n",
    "def get_patents_by_first_filing_year() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query raw_patents first_filing_year and aggregate.\n",
    "    (Same idea as your /api/patents/first_filing_years route.)\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT first_filing_year::int AS year, COUNT(*)::int AS count\n",
    "        FROM raw_patents\n",
    "        WHERE first_filing_year IS NOT NULL\n",
    "        GROUP BY first_filing_year\n",
    "        ORDER BY year\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql, engine)\n",
    "\n",
    "def truncate_tail_years(df: pd.DataFrame, tail: int, year_col=\"year\"):\n",
    "    if df.empty or tail <= 0:\n",
    "        return df.copy()\n",
    "    max_year = int(df[year_col].max())\n",
    "    cutoff = max_year - tail  # keep <= cutoff\n",
    "    return df[df[year_col] <= cutoff].copy()\n",
    "\n",
    "def fill_missing_years(df: pd.DataFrame, year_col=\"year\", count_col=\"count\"):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    years = pd.Series(sorted(df[year_col].unique()))\n",
    "    yr_min, yr_max = int(years.min()), int(years.max())\n",
    "    full = pd.DataFrame({year_col: list(range(yr_min, yr_max + 1))})\n",
    "    out = full.merge(df[[year_col, count_col]], on=year_col, how=\"left\").fillna({count_col: 0})\n",
    "    out[count_col] = out[count_col].astype(int)\n",
    "    return out\n",
    "\n",
    "def best_pub_to_patent_lag(pub_series: pd.Series, pat_series: pd.Series, max_lag=5):\n",
    "    \"\"\"\n",
    "    Simple cross-correlation (same-year alignment baseline).\n",
    "    Returns (best_lag, corr_at_best_lag). Positive lag means patents lag pubs.\n",
    "    \"\"\"\n",
    "    # Align on common years\n",
    "    idx = pub_series.index.intersection(pat_series.index)\n",
    "    p = pub_series.loc[idx].astype(float)\n",
    "    a = pat_series.loc[idx].astype(float)\n",
    "    if len(p) < 3:\n",
    "        return 0, float(\"nan\")\n",
    "    best = (0, -1.0)  # (lag, corr)\n",
    "    for lag in range(0, max_lag + 1):\n",
    "        # Shift patents backward by 'lag' to compare pubs[t] vs patents[t+lag]\n",
    "        aligned = a.shift(-lag)\n",
    "        s = pd.concat([p, aligned], axis=1).dropna()\n",
    "        if len(s) < 3:\n",
    "            continue\n",
    "        corr = s.iloc[:, 0].corr(s.iloc[:, 1])\n",
    "        if corr is not None and corr > best[1]:\n",
    "            best = (lag, float(corr))\n",
    "    return best\n",
    "\n",
    "# ---------------------------\n",
    "# Main collection procedure\n",
    "# ---------------------------\n",
    "\n",
    "def collect_series(tech_terms=TECH_TERMS,\n",
    "                   patent_tail=PATENT_TAIL_TRUNC,\n",
    "                   pub_tail=PUB_TAIL_TRUNC):\n",
    "    port = load_backend_port()\n",
    "    base = f\"http://localhost:{port}\"\n",
    "    pubs_endpoint    = f\"{base}/api/scientific_search_merge\"\n",
    "    patents_endpoint = f\"{base}/api/search_ops\"\n",
    "\n",
    "    all_pubs_rows = []\n",
    "    all_pat_rows  = []\n",
    "    lag_rows      = []\n",
    "\n",
    "    for term in tech_terms:\n",
    "        print(f\"\\n=== Collecting for tech='{term}' ===\")\n",
    "\n",
    "        # --- Publications: call endpoint, then aggregate from DB\n",
    "        try:\n",
    "            print(\"Publications: calling /api/scientific_search_merge …\")\n",
    "            _ = post_json(pubs_endpoint, {\"query\": term})\n",
    "            # Immediately query DB (table gets overwritten per term)\n",
    "            pub_df = get_publications_by_year()\n",
    "            pub_df = truncate_tail_years(pub_df, pub_tail, \"year\")\n",
    "            pub_df = fill_missing_years(pub_df, \"year\", \"count\")\n",
    "            pub_df[\"tech\"] = term\n",
    "            pub_df = pub_df[[\"tech\", \"year\", \"count\"]].rename(columns={\"count\": \"pub_count\"})\n",
    "            all_pubs_rows.append(pub_df)\n",
    "            print(f\"  -> pubs years {pub_df['year'].min()}–{pub_df['year'].max()} (n={len(pub_df)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  !! publications failed for '{term}': {e}\")\n",
    "\n",
    "        # --- Patents: call endpoint, then aggregate from DB\n",
    "        try:\n",
    "            print(\"Patents: calling /api/search_ops …\")\n",
    "            payload = patents_query_payload(term)\n",
    "            _ = post_json(patents_endpoint, payload)\n",
    "            pat_df = get_patents_by_first_filing_year()\n",
    "            pat_df = truncate_tail_years(pat_df, patent_tail, \"year\")\n",
    "            pat_df = fill_missing_years(pat_df, \"year\", \"count\")\n",
    "            pat_df[\"tech\"] = term\n",
    "            pat_df = pat_df[[\"tech\", \"year\", \"count\"]].rename(columns={\"count\": \"patent_count\"})\n",
    "            all_pat_rows.append(pat_df)\n",
    "            print(f\"  -> pats years {pat_df['year'].min()}–{pat_df['year'].max()} (n={len(pat_df)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  !! patents failed for '{term}': {e}\")\n",
    "\n",
    "        # Optional: xcorr-based lag detection for this tech (uses overlapping years)\n",
    "        try:\n",
    "            if len(all_pubs_rows) and len(all_pat_rows):\n",
    "                pub_cur = all_pubs_rows[-1].set_index(\"year\")[\"pub_count\"]\n",
    "                pat_cur = all_pat_rows[-1].set_index(\"year\")[\"patent_count\"]\n",
    "                lag, corr = best_pub_to_patent_lag(pub_cur, pat_cur, max_lag=5)\n",
    "                lag_rows.append({\"tech\": term, \"best_lag_years\": int(lag), \"xcorr\": corr})\n",
    "                print(f\"  -> best pubs→patents lag: {lag}y (xcorr={corr:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  !! lag calc failed for '{term}': {e}\")\n",
    "\n",
    "    pubs_df    = pd.concat(all_pubs_rows, ignore_index=True) if all_pubs_rows else pd.DataFrame(columns=[\"tech\",\"year\",\"pub_count\"])\n",
    "    patents_df = pd.concat(all_pat_rows,  ignore_index=True) if all_pat_rows else pd.DataFrame(columns=[\"tech\",\"year\",\"patent_count\"])\n",
    "    lag_df     = pd.DataFrame(lag_rows) if lag_rows else pd.DataFrame(columns=[\"tech\",\"best_lag_years\",\"xcorr\"])\n",
    "\n",
    "    # Ensure integer year\n",
    "    for df in (pubs_df, patents_df):\n",
    "        if \"year\" in df.columns:\n",
    "            df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "    return pubs_df, patents_df, lag_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pubs_df, patents_df, lag_df = collect_series()\n",
    "\n",
    "    # Save artifacts (optional)\n",
    "    pubs_df.to_csv(\"pubs_df_collected.csv\", index=False)\n",
    "    patents_df.to_csv(\"patents_df_collected.csv\", index=False)\n",
    "    lag_df.to_csv(\"pubs_to_patents_best_lag.csv\", index=False)\n",
    "\n",
    "    print(\"\\nDone. Samples:\")\n",
    "    print(pubs_df.head())\n",
    "    print(patents_df.head())\n",
    "    print(lag_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7ec4b",
   "metadata": {},
   "source": [
    "#utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- utilities\n",
    "\n",
    "def _to_year_end(year_series: pd.Series) -> pd.Series:\n",
    "    # Ensure Dec-31 ds per year\n",
    "    return pd.PeriodIndex(year_series.astype(int), freq='Y').to_timestamp(how='end')\n",
    "\n",
    "def _align_and_corr(pat_s: pd.Series, pub_s: pd.Series, lag: int, detrend: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns Pearson correlation between patents_t and publications_{t-lag},\n",
    "    after optional detrending. Returns np.nan if not enough overlap.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'pat': pat_s, 'pub': pub_s}).copy()\n",
    "    df['pub'] = df['pub'].shift(lag)\n",
    "    df = df.dropna()\n",
    "\n",
    "    if len(df) < 3:  # too few points for safe corr\n",
    "        return np.nan\n",
    "\n",
    "    if detrend == 'diff':\n",
    "        df = df.diff().dropna()\n",
    "    elif detrend == 'pct':\n",
    "        df = df.pct_change().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    elif detrend == 'zscore':\n",
    "        df = (df - df.mean()) / df.std(ddof=0)\n",
    "\n",
    "    if len(df) < 3:\n",
    "        return np.nan\n",
    "\n",
    "    return df['pat'].corr(df['pub'])\n",
    "\n",
    "def infer_publication_lag_per_tech(\n",
    "    pubs_df: pd.DataFrame,\n",
    "    patents_df: pd.DataFrame,\n",
    "    max_lag: int = 5,\n",
    "    detrend: str = 'diff',\n",
    "    min_overlap: int = 8,\n",
    "    prefer_nonnegative: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each tech, pick the lag in [0..max_lag] maximizing Pearson corr between\n",
    "    patents_t and publications_{t-lag} (after detrending). Returns a DataFrame:\n",
    "    [tech, best_lag, best_corr, overlap_years].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    techs = sorted(set(pubs_df['tech']).intersection(set(patents_df['tech'])))\n",
    "\n",
    "    for tech in techs:\n",
    "        pat = patents_df.loc[patents_df['tech'] == tech, ['year', 'patent_count']].set_index('year')['patent_count']\n",
    "        pub = pubs_df.loc[pubs_df['tech'] == tech, ['year', 'pub_count']].set_index('year')['pub_count']\n",
    "\n",
    "        # Restrict to overlapping years generously (we’ll shift inside)\n",
    "        yrs = sorted(set(pat.index).intersection(set(pub.index)))\n",
    "        if len(yrs) < min_overlap:\n",
    "            # Not enough to infer reliably; default lag=1\n",
    "            results.append({'tech': tech, 'best_lag': 1, 'best_corr': np.nan, 'overlap_years': len(yrs)})\n",
    "            continue\n",
    "\n",
    "        best = {'lag': 0, 'corr': -np.inf}\n",
    "        for lag in range(0, max_lag + 1):\n",
    "            # build aligned frame to count overlap for this lag\n",
    "            df = pd.DataFrame({'pat': pat, 'pub': pub.shift(lag)}).dropna()\n",
    "            overlap = len(df)\n",
    "            if overlap < min_overlap:\n",
    "                continue\n",
    "            r = _align_and_corr(pat, pub, lag=lag, detrend=detrend)\n",
    "            if np.isnan(r):\n",
    "                continue\n",
    "            if prefer_nonnegative and r < 0:\n",
    "                continue\n",
    "            if r > best['corr']:\n",
    "                best = {'lag': lag, 'corr': r, 'overlap': overlap}\n",
    "\n",
    "        if best['corr'] == -np.inf:\n",
    "            # fallback if nothing met criteria\n",
    "            results.append({'tech': tech, 'best_lag': 1, 'best_corr': np.nan, 'overlap_years': len(yrs)})\n",
    "        else:\n",
    "            results.append({'tech': tech, 'best_lag': best['lag'], 'best_corr': best['corr'], 'overlap_years': best['overlap']})\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf328bf5",
   "metadata": {},
   "source": [
    "#integrate the lag into prophet prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aec18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pub_regressor_column(\n",
    "    patents_df: pd.DataFrame,\n",
    "    pubs_df: pd.DataFrame,\n",
    "    lag_map: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges a lagged publication regressor named 'pub_reg' into the patents_df per tech.\n",
    "    Expects patents_df: [tech, year, patent_count]\n",
    "            pubs_df:    [tech, year, pub_count]\n",
    "    lag_map: {tech: best_lag}\n",
    "    Returns a tidy DF with columns: tech, year, patent_count, pub_reg\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for tech, g_pat in patents_df.groupby('tech'):\n",
    "        lag = int(lag_map.get(tech, 1))\n",
    "        g_pub = pubs_df[pubs_df['tech'] == tech].copy()\n",
    "        g_pub = g_pub.assign(year=lambda d: d['year'].astype(int) + lag)  # shift forward so that at year t, pub_reg = pubs_{t-lag}\n",
    "        g_pub = g_pub.rename(columns={'pub_count': 'pub_reg'})[['year', 'pub_reg']]\n",
    "\n",
    "        m = g_pat.merge(g_pub, on='year', how='left')\n",
    "        out.append(m.assign(tech=tech))\n",
    "    return pd.concat(out, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a957d9",
   "metadata": {},
   "source": [
    "during training & future construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "def _prep_prophet_df_for_patents(df_tech: pd.DataFrame) -> pd.DataFrame:\n",
    "    # df_tech: [year, patent_count, pub_reg]\n",
    "    df = df_tech.copy()\n",
    "    df['ds'] = _to_year_end(df['year'])\n",
    "    df = df.rename(columns={'patent_count': 'y'})\n",
    "    # Prophet requires regressor present in both history and future\n",
    "    return df[['ds', 'y', 'pub_reg']].dropna(subset=['y', 'pub_reg'])  # ensure no NaNs\n",
    "\n",
    "def forecast_patents_with_regressor_auto_lag(\n",
    "    patents_df: pd.DataFrame,\n",
    "    pubs_df: pd.DataFrame,\n",
    "    pub_forecasts: dict | None,\n",
    "    horizon: int,\n",
    "    lag_table: pd.DataFrame\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    per-tech Prophet using auto-selected lag. If `pub_forecasts` is None, caller must pass a\n",
    "    pubs scenario df per tech in same schema as pubs_df (years extended).\n",
    "    Returns: dict[tech] -> forecast DataFrame with columns ['ds','yhat','yhat_lower','yhat_upper']\n",
    "    \"\"\"\n",
    "    lag_map = dict(zip(lag_table['tech'], lag_table['best_lag']))\n",
    "    # Build training table with 'pub_reg'\n",
    "    train_tbl = add_pub_regressor_column(patents_df, pubs_df, lag_map)\n",
    "    forecasts = {}\n",
    "\n",
    "    for tech, g in train_tbl.groupby('tech'):\n",
    "        dfp = _prep_prophet_df_for_patents(g[['year', 'patent_count', 'pub_reg']])\n",
    "        if len(dfp) < 5:\n",
    "            continue  # not enough data to fit\n",
    "\n",
    "        m = Prophet(yearly_seasonality=False, daily_seasonality=False)\n",
    "        m.add_regressor('pub_reg')\n",
    "        m.fit(dfp)\n",
    "\n",
    "        # build future years\n",
    "        last_year = int(g['year'].max())\n",
    "        future_years = [last_year + i for i in range(1, horizon+1)]\n",
    "        future = pd.DataFrame({'year': future_years})\n",
    "\n",
    "        # build future pub_reg: use pub forecasts (or scenario) then shift by lag\n",
    "        lag = lag_map.get(tech, 1)\n",
    "        if pub_forecasts is not None and tech in pub_forecasts:\n",
    "            fut_pub = pub_forecasts[tech].copy()  # expected columns: ['year','pub_count']\n",
    "        else:\n",
    "            raise ValueError(\"Provide pub_forecasts per tech or implement scenario generator.\")\n",
    "\n",
    "        fut_pub_shifted = fut_pub.copy()\n",
    "        fut_pub_shifted['year'] = fut_pub_shifted['year'].astype(int) + lag\n",
    "        fut_pub_shifted = fut_pub_shifted.rename(columns={'pub_count': 'pub_reg'})[['year','pub_reg']]\n",
    "\n",
    "        future = future.merge(fut_pub_shifted, on='year', how='left')\n",
    "        future['ds'] = _to_year_end(future['year'])\n",
    "        future = future[['ds', 'pub_reg']]\n",
    "\n",
    "        # concatenate with history to let Prophet make future df\n",
    "        # Prophet needs the regressor in 'future' frame we pass to predict\n",
    "        fcst = m.predict(future)\n",
    "        forecasts[tech] = fcst[['ds','yhat','yhat_lower','yhat_upper']].copy()\n",
    "\n",
    "    return forecasts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773fb68",
   "metadata": {},
   "source": [
    "publications forecast + scenario hooks (unchanged , but shown for completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0062a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_publications_per_tech(pubs_df: pd.DataFrame, horizon: int) -> dict:\n",
    "    out = {}\n",
    "    for tech, g in pubs_df.groupby('tech'):\n",
    "        df = g[['year','pub_count']].copy()\n",
    "        df['ds'] = _to_year_end(df['year'])\n",
    "        df = df.rename(columns={'pub_count':'y'})\n",
    "        m = Prophet(yearly_seasonality=False, daily_seasonality=False)\n",
    "        m.fit(df[['ds','y']])\n",
    "        future = m.make_future_dataframe(periods=horizon, freq='Y')\n",
    "        fcst = m.predict(future)\n",
    "        # Extract future years as integer for downstream merging\n",
    "        tmp = fcst[['ds','yhat']].copy()\n",
    "        tmp['year'] = tmp['ds'].dt.year\n",
    "        out[tech] = tmp.rename(columns={'yhat':'pub_count'})[['year','pub_count']]\n",
    "    return out\n",
    "\n",
    "def generate_pub_scenario(pub_hist: pd.DataFrame, horizon: int, method='flat'):\n",
    "    last_year = int(pub_hist['year'].max())\n",
    "    hist = pub_hist.sort_values('year')\n",
    "    if method == 'flat':\n",
    "        val = hist['pub_count'].iloc[-1]\n",
    "        fut = [val]*horizon\n",
    "    elif method == 'linear_trend':\n",
    "        if len(hist) >= 6:\n",
    "            slope = (hist['pub_count'].iloc[-1] - hist['pub_count'].iloc[-6]) / 5.0\n",
    "        else:\n",
    "            slope = 0.0\n",
    "        start = hist['pub_count'].iloc[-1]\n",
    "        fut = [start + slope*(i+1) for i in range(horizon)]\n",
    "    years = [last_year + i for i in range(1, horizon+1)]\n",
    "    return pd.DataFrame({'year': years, 'pub_count': fut})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e48d62",
   "metadata": {},
   "source": [
    "evaluation sketch (showing where auto-lag plugs in )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a41267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def evaluate_prophet_models_with_auto_lag(\n",
    "    pubs_df: pd.DataFrame,\n",
    "    patents_df: pd.DataFrame,\n",
    "    horizon: int = 5,\n",
    "    test_years: int = 5,\n",
    "    max_lag: int = 5,\n",
    "    pub_trunc: int = 1,\n",
    "    pat_trunc: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    # assume truncation already applied during build; if not, apply here\n",
    "\n",
    "    # split train/test by year per tech AFTER truncation\n",
    "    metrics = []\n",
    "    for tech in sorted(set(pubs_df['tech']).intersection(patents_df['tech'])):\n",
    "        g_pub = pubs_df[pubs_df['tech']==tech].copy()\n",
    "        g_pat = patents_df[patents_df['tech']==tech].copy()\n",
    "        cutoff = int(min(g_pub['year'].max(), g_pat['year'].max()) - test_years)\n",
    "        pub_train, pub_test = g_pub[g_pub['year']<=cutoff], g_pub[g_pub['year']>cutoff]\n",
    "        pat_train, pat_test = g_pat[g_pat['year']<=cutoff], g_pat[g_pat['year']>cutoff]\n",
    "\n",
    "        # 1) publications forecast (train only)\n",
    "        pub_fcsts = forecast_publications_per_tech(pub_train, horizon=len(pub_test))\n",
    "        pub_pred = pub_fcsts.get(tech, pd.DataFrame(columns=['year','pub_count']))\n",
    "        pub_pred = pub_pred[pub_pred['year'].isin(pub_test['year'])]\n",
    "        if len(pub_pred):\n",
    "            mae_pub = mean_absolute_error(pub_test['pub_count'].values, pub_pred['pub_count'].values)\n",
    "            rmse_pub = np.sqrt(mean_squared_error(pub_test['pub_count'].values, pub_pred['pub_count'].values))\n",
    "        else:\n",
    "            mae_pub = rmse_pub = np.nan\n",
    "\n",
    "        # 2) auto-lag inference on TRAIN ONLY\n",
    "        lag_tab = infer_publication_lag_per_tech(pub_train, pat_train, max_lag=max_lag)\n",
    "        lag_map = dict(zip(lag_tab['tech'], lag_tab['best_lag']))\n",
    "\n",
    "        # 3) patents baseline (no regressor)\n",
    "        # fit simple Prophet on patents only (train)\n",
    "        dfp = pat_train[['year','patent_count']].copy()\n",
    "        dfp['ds'] = _to_year_end(dfp['year'])\n",
    "        dfp = dfp.rename(columns={'patent_count':'y'})\n",
    "        m_base = Prophet(yearly_seasonality=False, daily_seasonality=False)\n",
    "        m_base.fit(dfp[['ds','y']])\n",
    "        fut = pd.DataFrame({'ds': _to_year_end(pat_test['year'])})\n",
    "        base_pred = m_base.predict(fut)\n",
    "        base_yhat = base_pred['yhat'].values\n",
    "        mae_base = mean_absolute_error(pat_test['patent_count'].values, base_yhat)\n",
    "        rmse_base = np.sqrt(mean_squared_error(pat_test['patent_count'].values, base_yhat))\n",
    "\n",
    "        # 4) patents with regressor (auto-lag)\n",
    "        # build regressor on TRAIN, use pub forecast (from step 1) for TEST → then shift\n",
    "        # construct per-tech pubs future by concatenating pub_train with pub_pred for alignment\n",
    "        pub_future = pd.concat([pub_train[['year','pub_count']], pub_pred[['year','pub_count']]], ignore_index=True)\n",
    "        lagged_train = add_pub_regressor_column(pat_train, pub_train, lag_map)\n",
    "        df_train = _prep_prophet_df_for_patents(lagged_train[['year','patent_count','pub_reg']])\n",
    "\n",
    "        m = Prophet(yearly_seasonality=False, daily_seasonality=False)\n",
    "        m.add_regressor('pub_reg')\n",
    "        m.fit(df_train)\n",
    "\n",
    "        lag = lag_map.get(tech, 1)\n",
    "        pub_future_shift = pub_future.copy()\n",
    "        pub_future_shift['year'] = pub_future_shift['year'].astype(int) + lag\n",
    "        pub_future_shift = pub_future_shift.rename(columns={'pub_count':'pub_reg'})[['year','pub_reg']]\n",
    "\n",
    "        fut_pat = pd.DataFrame({'year': pat_test['year']})\n",
    "        fut_pat = fut_pat.merge(pub_future_shift, on='year', how='left')\n",
    "        fut_pat['ds'] = _to_year_end(fut_pat['year'])\n",
    "        fc = m.predict(fut_pat[['ds','pub_reg']])\n",
    "        mae_reg = mean_absolute_error(pat_test['patent_count'].values, fc['yhat'].values)\n",
    "        rmse_reg = np.sqrt(mean_squared_error(pat_test['patent_count'].values, fc['yhat'].values))\n",
    "\n",
    "        metrics.append({\n",
    "            'tech': tech,\n",
    "            'best_lag': lag,\n",
    "            'mae_pubs': mae_pub, 'rmse_pubs': rmse_pub,\n",
    "            'mae_patents_with_reg': mae_reg, 'rmse_patents_with_reg': rmse_reg,\n",
    "            'mae_patents_baseline': mae_base, 'rmse_patents_baseline': rmse_base\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
